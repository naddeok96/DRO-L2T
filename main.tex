\documentclass[11pt]{article}

% ---------- Preamble ----------
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{xspace}

\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=blue,
  urlcolor=blue
}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\newcommand{\MacroAcc}{\mathrm{MacroAcc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\Prb}{\mathbb{P}}
% --- Acronym and macro ---
\newcommand{\MAESTRO}{\textsc{MAESTRO}\xspace}
% MAESTRO: Markovian, Architecture-Agnostic, Equitable Scheduling for Task-Robust Optimization

\title{\textbf{Learning to Teach for Distributional Robustness: \\Reinforcement Learning for Universal Curriculum Design Across Tasks and Datasets}}

\author{Anonymous Authors}
\date{}

% ---------- Document ----------
\begin{document}
\maketitle

\begin{abstract}
In practice, models intended for a small, high-stakes domain are trained alongside much larger auxiliary datasets. This “many-to-one” regime accelerates learning but can mask failure on the target domain: for example, a mixture in which roughly $85$–$90\%$ of training examples come from a dominant source and the remaining $10$–$15\%$ are spread across specialized datasets can yield strong average accuracy while performing poorly on the minority slices that matter. We revisit \emph{learning to teach} and design \MAESTRO{ (\emph{Markovian, Architecture-Agnostic, Equitable Scheduling for Task-Robust Optimization})}, a reinforcement-learning teacher that explicitly guards against this imbalance by adaptively controlling three levers of the curriculum: the per-dataset mixture, the learning rate, and the lesson size (budget usage). Our formulation makes the teacher–student interaction \emph{Markov} by grounding transitions in SGD dynamics and exposing a compact, task/architecture-agnostic observation with three groups: (A) \textbf{dataset-level DeepSets embeddings} computed from permutation-invariant probe statistics (loss/uncertainty summaries, gradient-norm moments, and diversity sketches), (B) a \textbf{model-complexity} vector, and (C) a \textbf{training-progress} vector. A set-based encoder with mean pooling and shared per-dataset policy heads ensures \emph{number-of-datasets invariance}, producing valid mixtures for any number of datasets and any permutation, while a constrained-MDP view manages total budget. We validate \emph{approximate Markovity} via one-step prediction $R^2$ with only small gains from short histories, demonstrate invariance under varying number and permutations, and show cross-task/architecture portability (e.g., classification, detection, sequence labeling, QA), including leave-one-family-out and zero/few-shot transfer from toy episodes to large models. Despite its compactness, \MAESTRO{} achieves strong sample efficiency and improves balanced metrics (macro metrics) across heterogeneous mixtures.


\end{abstract}

\section{Introduction}
In production pipelines, a model intended for a narrow, high-stakes domain is often trained with help from many auxiliary datasets. This “many-to-one” regime is pragmatic—large sources accelerate learning—but brittle: when evaluation or deployment hinges on a small curated dataset, optimizing an aggregate objective can obscure failures on that minority slice. In extremis, if a single dataset contributes $99\%$ of all training examples, a seemingly strong mean accuracy can coexist with poor performance on the $1\%$ that matters. This phenomenon mirrors the \emph{repeated loss minimization} and \emph{group shift} observations in the fairness and robustness literature: naively minimizing average risk can systematically underserve minority groups \citep{hashimoto2018fairness,sagawa2020dro}.

We study \emph{learning to teach}: a reinforcement-learning (RL) \emph{teacher} that observes a student’s competence and adaptively decides \emph{which datasets} to emphasize, \emph{how aggressively} to spend budget, and \emph{what learning rate} to use. The goal is to maximize balanced end-of-training metrics—e.g., \emph{macro-accuracy} across any number of datasets, so that small, specialized datasets are not eclipsed by large, easy ones. Here, “macro” denotes the equal-weight average of per-dataset accuracies, irrespective of dataset size. We refer to our teacher as \MAESTRO{} (\emph{Markovian, Architecture-Agnostic, Equitable Scheduling for Task-Robust Optimization}).

A central challenge is engineering the teacher’s interface so that it is (1) \emph{Markov}, enabling stable RL control; (2) \emph{invariant in the number of datasets}, so one policy naturally scales to any number of datasets and any permutation; and (3) \emph{portable} across task families and architectures, so a single teacher trained on small, inexpensive episodes can be deployed in varied settings. Existing learning-to-teach approaches demonstrate the promise of dynamic data selection and learned losses \citep{fan2018l2t,wu2018l2tloss}, yet typically do not combine a formal Markov interface with a \emph{set-based} design for many datasets and a compact, task-agnostic observation that can transfer broadly. Meanwhile, results in group-robust optimization argue for objectives that explicitly guard minority performance \citep{hashimoto2018fairness,sagawa2020dro}; we bring that spirit to curriculum design by reallocating training mixture and budget over time.

\textbf{Our approach.} We formalize the hidden state of training under SGD and construct a grouped, task/architecture/optimizer-agnostic observation with three components: \emph{(A) dataset-level embeddings} built via DeepSets from small, fixed probes (loss/uncertainty histograms, gradient-norm moments, diversity sketches, light meta-statistics), \emph{(B) model-complexity} summaries, and \emph{(C) training-progress} dynamics. A set-based encoder with mean pooling ensures permutation and cardinality invariance; shared per-dataset policy heads emit mixture logits $w_t$ for any number of datasets. Additional heads control learning rate and usage, allowing the teacher to modulate \emph{both} exposure and step size. We adopt a CMDP view over budget. We denote this unified design by \MAESTRO{}. 

\textbf{Contributions.} (1) We introduce \MAESTRO{}, a Markov teacher--student formulation with a \emph{grouped, universal state} (dataset embeddings + model complexity + training progress) that is compact yet empirically \emph{approximately} Markov. (2) A \emph{set-invariant} policy/value parameterization whose per-dataset heads and mean-pooled encodings produce valid mixtures for any number of datasets and any permutation. (3) A meta-training/evaluation protocol demonstrating \emph{portability} across task families and architectures, including leave-one-family-out and zero-/few-shot transfer. (4) A CMDP treatment that jointly controls mixture, learning rate, and usage, improving balanced outcomes (macro metrics) over uniform sampling, myopic/bandit schedulers, and heuristic curricula. Conceptually, \MAESTRO{} operationalizes the group-robust principle—do not ignore small groups—through an adaptive curriculum rather than a static objective.

\textbf{Why this matters.} Practitioners increasingly face mixtures where large auxiliary datasets threaten to dominate optimization while deployment risk concentrates on a small, specialized dataset. A teacher that is \emph{Markov}, \emph{set-invariant}, and \emph{task/architecture-invariant} can be trained once and reused, reallocating budget toward underperforming datasets as training unfolds. Our results suggest that a compact, grouped observation suffices to steer learning dynamics toward balanced performance, complementing distributionally robust objectives with a procedural lever: the curriculum itself.

\section{Related Work}
\paragraph{Learning to teach and curriculum RL.}
Classic curriculum and self-paced learning emphasize the sequencing of examples \citep{bengio2009curriculum,kumar2010selfpaced}. \citet{fan2018l2t} frame \emph{Learning to Teach} as RL, selecting instances to present and reporting cross-architecture generalization. \citet{wu2018l2tloss} learn dynamic loss functions to guide optimization. These lines establish that teachers can shape learning trajectories, but typically focus on per-example selection or loss design \emph{without} a formal Markov interface, an explicitly \emph{set-invariant} treatment of many datasets, or a compact, task-agnostic observation sufficient for transfer across modalities.

\paragraph{Fairness and group-robust objectives.}
Optimizing average risk can degrade minority-group performance; \citet{hashimoto2018fairness} formalize this phenomenon in repeated loss minimization and propose a robust objective, and \citet{sagawa2020dro} analyze distributionally robust neural networks under group shifts, highlighting regularization for worst-group performance. We share the goal of protecting small groups, but tackle it \emph{procedurally}: by adaptively reweighting exposure (mixture), learning rate, and budget over time, rather than by altering the static loss. Our teacher can be layered atop standard objectives, widening applicability.

\paragraph{Multi-task scheduling and loss weighting.}
Methods that weight losses or balance gradients \citep{kendall2018uncertainty,chen2018gradnorm} influence multi-task outcomes, yet do not explicitly reason about Markov state, budgeted control, or variable-cardinality sets. Our approach is complementary: we control \emph{which} data the student sees and \emph{how much} to train at each step, while such methods can still govern per-task losses.

\paragraph{Meta-learning and RL controllers.}
Meta-RL \citep{duan2016rl2,finn2017maml} and hyperparameter controllers (PBT/BOHB) \citep{jaderberg2017pbt,falkner2018bohb,snoek2012practical} optimize learning procedures or schedules. We differ in providing (i) a compact, task-agnostic, \emph{group-sensitive} observation; (ii) \emph{set-invariant} state/action encodings for any number of datasets; and (iii) an explicit CMDP view of budget, enabling balanced control of exposure and learning rate.

\paragraph{Permutation-invariant encoders for sets.}
DeepSets \citep{zaheer2017deepsets} and Set Transformer \citep{lee2019settransformer} provide universal forms for permutation-invariant/equivariant processing. We adopt these to encode dataset-level statistics and to parameterize per-dataset action heads, ensuring that the teacher’s decisions are valid and stable for \emph{any} number and ordering of datasets.

\medskip
\noindent\textbf{Position relative to prior work.}
Compared to \citet{fan2018l2t,wu2018l2tloss}, we (i) formalize a Markov teacher--student MDP and empirically diagnose \emph{approximate Markovity} of a compact \emph{grouped} observation; (ii) enforce \emph{number-of-datasets invariance} with set encoders and shared per-dataset heads; and (iii) target \emph{task/architecture invariance} via task-agnostic statistics (probabilities, losses, gradient magnitudes, diversity sketches) plus model/progress blocks. Relative to fairness and GroupDRO \citep{hashimoto2018fairness,sagawa2020dro}, we provide a complementary mechanism—\emph{adaptive curricula}—that can protect minority datasets without modifying the base loss.

\section{Problem Formulation}
We consider $N$ datasets $\{\mathcal{D}_i\}_{i=1}^N$ drawn from an episode-specific task $\tau\sim\mathcal{T}$. A student model $f_\theta$ is trained under a teacher policy $\pi_\phi$ over horizon $H$ with initial budget $B_0$. Our objective emphasizes \emph{balanced} end performance (macro- or worst-dataset metrics), reflecting the practical risk that small, high-stakes datasets be overshadowed by large auxiliaries.

\subsection{Markov Teacher--Student MDP}\label{sec:mdp}
\paragraph{Definition (Markov property).}
An RL environment with observable state $S_t$ and action $a_t$ is Markov if
\begin{equation}
\Prb(S_{t+1},r_t \mid S_t,a_t) \;=\; \Prb(S_{t+1},r_t \mid S_t,a_t,S_{t-1},a_{t-1},\dots).
\label{eq:markov}
\end{equation}

\paragraph{Environment dynamics.}
We write $g$ for the (possibly stochastic) transition mechanism that, together with the observation map, induces the law of $(S_{t+1},r_t)$ given $(S_t,a_t)$.

\begin{definition}[Rich predictor class]\label{def:rich-class}
A predictor class $\mathcal{F}$ is \emph{rich} if it can approximate the target mappings considered to arbitrary accuracy in $L^2$; e.g., universal-approximation neural nets on compact domains are sufficient for our purposes.
\end{definition}

\begin{definition}[Approximate Markovity]
\label{def:approx-markov-tight}
Let $\mathcal{F}$ be a rich predictor class (Def.~\ref{def:rich-class}). Define
$\mathcal{E}_0 \triangleq \inf_{F\in\mathcal{F}} \E\| S_{t+1} - F(S_t,a_t)\|^2$
and
$\mathcal{E}_1 \triangleq \inf_{F\in\mathcal{F}} \E\| S_{t+1} - F(S_t,a_t,S_{t-1},a_{t-1})\|^2$.
We say the environment dynamics $g$ are $(\varepsilon,\delta)$-Markov if $\mathcal{E}_0 \le \varepsilon$ and
$\mathcal{E}_0 - \mathcal{E}_1 \le \delta$ with $\delta \ll \varepsilon$.
Empirically, we report the held-out improvement
$\Delta R^2 \triangleq R^2\!\big(S_{t+1}\!\leftarrow\!(S_t,a_t,S_{t-1},a_{t-1})\big) - R^2\!\big(S_{t+1}\!\leftarrow\!(S_t,a_t)\big)$,
averaging componentwise if $S_{t+1}$ is vector-valued.
\end{definition}

\begin{definition}[\MAESTRO: Markovian, Architecture-Agnostic, Equitable Scheduling for Task-Robust Optimization]\label{def:maestro}
\MAESTRO{} is a reinforcement-learning teacher that controls curriculum via actions
$a_t=(w_t,\ell_t,u_t)$ comprising per-dataset mixture weights $w_t$, learning rate $\ell_t$, and budget usage $u_t$.
It operates on a compact, task/architecture/optimizer-agnostic \emph{grouped} observation
$S_t=\big(\{z_{t,i}\}_{i=1}^N,\,g^{\mathrm{model}}_t,\,g^{\mathrm{progress}}_t\big)$,
where $\{z_{t,i}\}$ are permutation-invariant dataset embeddings computed from fixed probe statistics, and $g^{\mathrm{model}}_t$, $g^{\mathrm{progress}}_t$ summarize capacity and training dynamics.
The policy/value encoders are set-invariant (DeepSets with mean pooling) and share per-dataset heads so that mixture decisions remain valid for any number of datasets and any permutation.
The objective emphasizes \emph{equitable scheduling} toward task-robust performance, optimizing balanced metrics such as macro-accuracy or worst-dataset accuracy under a constrained-MDP budget.
\end{definition}

\paragraph{Hidden state and SGD dynamics.}
Let the \emph{hidden} state be
\(
x_t \triangleq (\theta_t,\,B_t,\,\tau),
\)
where $\theta_t$ are student parameters, $B_t$ the remaining budget, and $\tau$ task parameters (e.g., margins, noise, imbalances).
Given an action $a_t=(w_t,\ell_t,u_t)$---mixture over datasets, learning rate, and usage fraction---the student samples with \emph{replacement} $m_t=\lfloor u_t B_t\rfloor$ examples from the mixture $\sum_i w_{t,i}\mathcal{D}_i^\tau$ and performs $K_t=\lceil m_t/b\rceil$ SGD steps (batch size $b$) with step size $\ell_t$:
\begin{equation}
\theta_{t+1} \;=\; U(\theta_t; a_t, \tau, \xi_t),\qquad B_{t+1}=B_t-m_t,
\end{equation}
where $\xi_t$ denotes minibatch randomness.
Sampling at $t$ thus depends only on $(x_t,a_t)$.
\textit{Remark:} If drawing \emph{without} replacement from finite pools, augment $x_t$ with per-dataset remaining counts so the process remains Markov; we otherwise assume with-replacement sampling in experiments.
\begin{assumption}[With-replacement sampling]\label{assump:with-repl}
Unless stated otherwise, mini-batches at time $t$ are sampled \emph{with replacement} from the mixture $\sum_i w_{t,i}\mathcal{D}_i^\tau$. This makes the data-generation mechanism conditionally independent of history given $(x_t,a_t)$.
\end{assumption}

\paragraph{Observable state (grouped, task/model/optimizer-agnostic).}
We decompose the observable state into three blocks and concatenate them for control:
\[
S_t \;=\; \Big(\;\underbrace{\{z_{t,i}\}_{i=1}^N}_{\text{dataset-level set}}\;,\;
\underbrace{g^{\text{model}}_t}_{\text{model complexity}}\;,\;
\underbrace{g^{\text{progress}}_t}_{\text{training progress}}\;\Big).
\]
\textbf{(A) Dataset-level DeepSets embeddings.} For each dataset $i$, compute a permutation-invariant embedding
\[
z_{t,i} \;=\; \mathrm{DeepSets}\!\Big(\{\text{probe stats from } \mathcal{D}_i\}\Big)\in\R^{d_z},
\]
where the input set contains only task-agnostic statistics computed on a tiny, fixed probe slice each step:
\begin{itemize}[leftmargin=1.25em]
  \item \emph{Predictive performance \& uncertainty:} macro-Acc/F1 (when labels available), mean NLL (length-normalized for sequences), confidence percentiles $(p_{10},p_{50},p_{90})$, ECE-5.
  \item \emph{Loss/difficulty:} loss histogram or quantiles (8 bins).
  \item \emph{Gradient-based difficulty:} mean/std of per-example gradient norms w.r.t. logits (log-scale).
  \item \emph{Diversity sketch:} embedding variance or SimHash/Sketch counts from a small reservoir.
  \item \emph{Light meta:} $\log$ dataset size and label entropy.
\end{itemize}
All quantities are standardized using robust EMAs (median/IQR) and are architecture- and label-permutation agnostic, following the spirit of \citet{fan2018l2t} (student feedback on samples) while remaining model-agnostic.

\begin{definition}[Grouped state: minimal-viable instantiation]\label{def:minimal}
At each step $t$, the observable grouped state is
$
S_t=\big(\{z_{t,i}\}_{i=1}^N,\,g^{\mathrm{model}}_t,\,g^{\mathrm{progress}}_t\big)
$,
where, for each dataset $i$:
\begin{align*}
z_{t,i} &:= \mathrm{DeepSets}\!\Big(\{\text{loss histogram (8 bins)},\ \text{confidence percentiles }(p_{10},p_{50},p_{90}),\\
&\qquad\ \ \text{ECE-5},\ \text{mean NLL (length-normalized)},\ \text{grad-norm mean/std (log)},\ \text{diversity sketch},\ \log|\mathcal{D}_i|,\ \text{label entropy}\}\Big)\in\R^{d_z},\\
g^{\mathrm{model}}_t &:= \big[\log|\theta|,\ \log\text{FLOPs/forward},\ \text{overfit gap}=\text{loss}_{\text{train}}-\text{loss}_{\text{val}},\ \text{LR},\ \text{momentum or }(\beta_1,\beta_2),\ \text{weight decay}\big],\\
g^{\mathrm{progress}}_t &:= \big[t/H,\ B_t/B_0,\ \mathrm{EMA}(\text{train loss}),\ \mathrm{EMA}(\text{val loss}),\ \Delta\text{loss/step},\ \text{gen gap},\ \text{grad-norm mean/std},\\
&\qquad\ \ \cos\angle(\nabla_t,\nabla_{t-1}),\ \text{LR}/\text{LR}_0\big].
\end{align*}
All scalars are robustly normalized (median/IQR clipping). When labels on the probe are unavailable, macro-Acc/F1 terms are omitted and proper scores (NLL, calibration) are used.
\end{definition}

\textbf{(B) Model-complexity block.} A small vector
\[
g^{\text{model}}_t=\big[\log|\theta|,\;\log\text{FLOPs/forward},\;\text{overfit gap}=\text{loss}_{\text{train}}-\text{loss}_{\text{val}},\;\text{LR},\;\text{momentum or }(\beta_1,\beta_2),\;\text{weight decay}\big].
\]
This captures capacity and optimizer settings without accessing architecture internals.

\textbf{(C) Training-progress block.} A small vector
\[
g^{\text{progress}}_t=\big[t/H,\;B_t/B_0,\;\text{EMA(train loss)},\;\text{EMA(val loss)},\;\Delta\text{loss/step},\;\text{gen gap},\;\text{grad-norm mean/std},\;\cos\angle(\nabla_t,\nabla_{t-1}),\;\text{LR}/\text{LR}_0\big].
\]
These dynamics features approximate the transition-relevant information needed for Markov control.

\paragraph{Confidence calibration (ECE-5).}
We compute a 5-bin expected calibration error on the maximum predicted class probability $p_{\max}$.
Let $\mathcal{B}_b$ be instances with $p_{\max}\in((b-1)/5,b/5]$. Then
$\mathrm{ECE}_5=\sum_{b=1}^5 \frac{|\mathcal{B}_b|}{n}\,\big|\mathrm{acc}(\mathcal{B}_b)-\mathrm{conf}(\mathcal{B}_b)\big|$,
where $\mathrm{conf}(\mathcal{B}_b)$ is the average $p_{\max}$ and $\mathrm{acc}(\mathcal{B}_b)$ the empirical accuracy.
Here $n$ is the number of evaluated instances (tokens for sequences; matched boxes for detection). For regression-style probes, ECE-5 is replaced by a proper calibration score (e.g., empirical coverage error under a Gaussian head).

\begin{proposition}[Hidden-state Markovity]
Under SGD with batches drawn according to $w_t$ \emph{with replacement} and fixed update rule $U$, the process on hidden states satisfies
\(
\Prb(x_{t+1},r_t \mid x_t,a_t)=\Prb(x_{t+1},r_t \mid x_t,a_t,x_{t-1},a_{t-1},\dots).
\)
\end{proposition}
\begin{proof}[Proof sketch]
Sampling at time $t$ depends only on $(x_t,a_t)$; the SGD update is a measurable function of $(\theta_t,a_t,\tau,\xi_t)$.
Therefore $(x_{t+1},r_t)$ depends on history only through $(x_t,a_t)$.
If sampling without replacement, include remaining counts in $x_t$.
\end{proof}

\begin{proposition}[Strict Markovity under information-preserving observation]\label{prop:inject}
If the observation map is injective (i.e., $S_t=g_{\mathrm{obs}}(x_t)$ preserves information), then the observable process is Markov:
\(
\Prb(S_{t+1},r_t \mid S_t,a_t)=\Prb(S_{t+1},r_t \mid S_t,a_t,S_{t-1},a_{t-1},\dots).
\)
\end{proposition}

\paragraph{Reward (macro-accuracy across datasets) and episode termination.}
Let $\mathrm{Acc}_{i,t}$ be validation accuracy on dataset $i$; macro-accuracy $\MacroAcc_t=\frac{1}{N}\sum_{i=1}^N \mathrm{Acc}_{i,t}$.
We use either terminal $r_t=\1\{t=H\}\MacroAcc_H$ or incremental $r_t=\MacroAcc_t-\MacroAcc_{t-1}$. In all cases, \MAESTRO{} targets equitable outcomes by directly optimizing balanced metrics.
To avoid reward shaping issues from stochastic early termination, we either forbid early termination (fixed $H$) or use terminal-only rewards and report $\MacroAcc_H$.

\paragraph{Budget as a CMDP.}
Budget consumption $c_t=m_t$ induces a constrained MDP
\(
\max_\pi \ \E[\sum_{t=1}^H r_t]\ \text{s.t.}\ \E[\sum_{t=1}^H c_t]\le B_0.
\)
We optimize the Lagrangian
\(
\mathcal{L}=\E\!\left[\sum_{t=1}^H (r_t-\lambda\,c_t)\right],
\)
with a per-episode dual update
\(
\lambda \leftarrow \big[\lambda + \eta_\lambda\big(\sum_{t=1}^H c_t - B_0\big)\big]_+.
\)

\subsection{Number-of-Datasets Invariance}\label{sec:invariance}
\paragraph{State as a set.}
Let the per-dataset embeddings at time $t$ be $\{z_{t,i}\}_{i=1}^N$.
We require the encoder to be \emph{permutation-invariant} and to admit variable cardinality $N$ without architectural changes or scale drift; \MAESTRO{} satisfies this via a DeepSets encoder with mean pooling.
By the DeepSets theorem \citep{zaheer2017deepsets}, any continuous permutation-invariant function $f$ on sets can be written as
\begin{equation}
f(\{z_{t,i}\})=\rho\!\left(\frac{1}{N}\sum_{i=1}^N \phi(z_{t,i})\right),
\label{eq:deepsets}
\end{equation}
for suitable $\phi$ and $\rho$ (universal approximators in practice).
We implement the policy/value encoders as shared per-dataset blocks $\phi$ followed by \emph{mean} pooling and a readout $\rho$, then concatenate $g^{\text{model}}_t$ and $g^{\text{progress}}_t$, making them invariant to dataset order and stable in $N$.

\paragraph{Action as per-dataset logits.}
For mixture control, the policy emits a scalar logit $\alpha_{t,i}$ for each dataset via a \emph{shared} head applied to every $z_{t,i}$:
\(
\alpha_{t,i} = h_{\text{mix}}(\phi(z_{t,i})),\quad
w_{t,i}=\frac{\exp(\alpha_{t,i})\,\1\{n_i>0\}}{\sum_{j=1}^N \exp(\alpha_{t,j})\,\1\{n_j>0\}},
\)
where $n_i$ is the available size for dataset $i$ if sampling without replacement. Under with-replacement sampling,
we drop the mask and set $w_t=\mathrm{softmax}(\{\alpha_{t,i}\})$. Shared heads and normalization over observed elements yield permutation equivariance and well-defined actions for any number of datasets.

\paragraph{Implication.}
The same learned policy applies to unseen numbers of datasets or new orderings without retraining; only the set $S_t$ changes in size or order, which the DeepSets encoder and per-element policy heads support natively.

\section{Method}
\subsection{Policy and Training}
We use an invariant encoder $f_\psi$ with a shared per-dataset block $\phi_\psi$ and \emph{mean}-pooled DeepSets over $\{z_{t,i}\}$ to produce a representation $h_t$, then concatenate the grouped globals $g^{\text{model}}_t$ and $g^{\text{progress}}_t$. Mean pooling (vs.\ sum) avoids scale drift as $N$ varies.
Policy heads output (i) per-dataset mixture logits $\{\alpha_{t,i}\}$, normalized to $w_t$, (ii) learning rate $\ell_t$ (bounded via squashing to $[\ell_{\min},\ell_{\max}]$), and (iii) usage $u_t\in(0,1)$ (sigmoid).
A value head $V_\psi(S_t)$ shares the invariant encoder and globals. This architecture instantiates \MAESTRO{} as defined in Def.~\ref{def:maestro}.

\paragraph{Action regularizers and barriers.}
To prevent collapse and boundary hugging, we add entropy and barrier regularizers to the PPO loss:
\(
\mathcal{L}_{\text{ent}}= \beta_{\text{mix}}\, H(w_t) + \beta_u\, H(u_t),\quad
\mathcal{L}_{\text{bar}}= \kappa\big[-\log(\ell_t-\ell_{\min})-\log(\ell_{\max}-\ell_t)\big] + \kappa'\big[-\log u_t - \log (1-u_t)\big],
\)
with standard clipping of pre-squash outputs for numerical stability.

\paragraph{Student update semantics (removing confounds).}
We fix batch size $b$.
Usage chooses a sample count $m_t=\lfloor u_t B_t\rfloor$, inducing $K_t=\lceil m_t/b\rceil$ optimizer steps with step size $\ell_t$.
Mini-batches are drawn \emph{with replacement} from the mixture with proportions $w_t$.
This separates the effects of step size and data volume; any residual coupling is tempered by $\mathcal{L}_{\text{bar}}$ and the CMDP penalty.

\begin{algorithm}[H]
\caption{PPO teacher on a Markov, set-valued curriculum MDP (updated)}
\label{alg:ppo}
\begin{algorithmic}[1]
\State Initialize policy/value params $\phi,\psi$
\For{iteration $=1,2,\dots$}
  \For{each parallel episode}
    \State Sample task $\tau\sim\mathcal{T}$; reset student; $B_1\leftarrow B_0$
    \For{$t=1$ to $H$}
      \State \Comment{build grouped state on fixed probes}
      \State For each dataset $i$, compute probe statistics and $z_{t,i}$; set $h_t=\rho\!\big(\tfrac{1}{N}\sum_i \phi(z_{t,i})\big)\,\Vert\,g^{\text{model}}_t\,\Vert\,g^{\text{progress}}_t$
      \State Emit $\{\alpha_{t,i}\}$, $\ell_t$, $u_t$; set $w_t=\mathrm{softmax}(\{\alpha_{t,i}\})$
      \State Apply $K_t$ SGD steps using batches drawn \emph{with replacement} from mixture $w_t$ with step size $\ell_t$ (\MAESTRO{})
      \State $m_t=\lfloor u_t B_t\rfloor$;\quad $B_{t+1}=B_t-m_t$
      \State (Optional, finite pools) re-mask any dataset with $n_i=0$
      \State Observe $(S_{t+1}, r_t)$
    \EndFor
  \EndFor
  \State Compute advantages; update $\phi,\psi$ with PPO and dual variable $\lambda$ (per-episode update)
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\subsection{Cross-Task Meta-Training and Evaluation}
\paragraph{Task families.}
We instantiate three families for meta-training: (i) classification (synthetic mixtures + small real subsets), (ii) sequence labeling (synthetic span tasks + small NER subsets), and (iii) detection (synthetic shapes with boxes + small real subsets). 
We construct episodes that randomize dataset difficulty, size, and noise. 
The teacher never observes task IDs; it only receives the grouped state (dataset embeddings + model/training blocks).

\paragraph{Leave-one-family-out (LOFO).}
We train the teacher on two families and evaluate zero-shot on the held-out family (e.g., train on classification+NER, test on detection), reporting macro metrics appropriate to each family (accuracy/F1, F1 for NER, mAP/AR for detection), and sample-efficiency AUC.

\paragraph{Large-model transfer.}
We stress-test transfer by deploying the teacher zero-shot to a large ViT-based detector trained over 10 detection datasets. 
The teacher controls dataset mixture, learning rate, and usage; the state is the same grouped observation computed on a fixed probe slice.
We report mAP@[.50:.95] and AP$_\mathrm{S/M/L}$.

\paragraph{Diagnostics.}
We measure (i) \emph{approximate Markovity}: one-step $R^2$ of $S_{t+1}$ from $(S_t,a_t)$ and the $\Delta R^2$ from adding history; (ii) \emph{$N$-invariance}: training at $N\in\{3,5\}$ and evaluating at $N\in\{2,4,7,10\}$ under random permutations; and (iii) entropy/barrier usage to verify stable control.

\paragraph{Baselines.}
Static Uniform, Easy$\to$Hard, Myopic Greedy (one-step gain surrogate), bandits (LinUCB/Thompson over datasets), and schedule controllers (PBT/BOHB).

\subsection{Validating Markovity}\label{sec:markov-exp}
We fit a one-step predictor $\hat{F}_\eta$ trained to minimize $\|S_{t+1}-\hat{F}_\eta(S_t,a_t)\|^2$ on logged rollouts; high $R^2$ supports approximate Markovity of the grouped state (Def.~\ref{def:approx-markov-tight}).
We split train/test \emph{by episode} to avoid leakage and report $R^2$ per feature, alongside a trivial ``no-change'' baseline.
We also test whether augmenting inputs with history $(S_{t-1},a_{t-1})$ improves $R^2$; a small $\Delta R^2$ indicates limited additional information in history.
\begin{table}[H]
\centering
\caption{One-step prediction on the grouped observation (placeholders). Higher is better.}
\label{tab:markov}
\begin{tabular}{lccc}
\toprule
Env & $R^2(S_t,a_t\!\to\!S_{t+1})$ & $\Delta R^2$ w/ history & GRU policy gain \\
\midrule
Gaussian & $0.00$ & $+0.00$ & $+0.00$ \\
Linear & $0.00$ & $+0.00$ & $+0.00$ \\
Shapes & $0.00$ & $+0.00$ & $+0.00$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Testing Number-of-Datasets Invariance}
We train teachers with $N\in\{3,5\}$ and evaluate zero-shot on $N\in\{2,4,7,10\}$, as well as under random dataset permutations at each episode start.
We report MacroAcc and permutation robustness $\sigma_\pi(\MacroAcc)$, the standard deviation across permutations of the same episode.
\begin{table}[H]
\centering
\caption{$N$-scaling and permutation robustness across tasks (placeholders).}
\label{tab:ninvariance}
\begin{tabular}{lccc}
\toprule
Train $N$ & Test $N$ & MacroAcc $\uparrow$ & Permutation $\sigma$ \\
\midrule
3 & 2 & 00.0 & 0.00 \\
3 & 7 & 00.0 & 0.00 \\
5 & 10 & 00.0 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Main Results (Placeholders)}
\begin{figure}[H]
  \centering
  \fbox{\rule{0pt}{2in}\rule{0.95\linewidth}{0pt}}
  \caption{Learning curves (MacroAcc vs. steps) in cross-task and LOFO evaluations.}
  \label{fig:indist}
\end{figure}

\begin{table}[H]
\centering
\caption{Final MacroAcc (\%) on cross-task and LOFO tests.}
\label{tab:indist}
\begin{tabular}{lc}
\toprule
Method & MacroAcc $\uparrow$ \\
\midrule
Static Uniform & 00.0 \\
Easy$\to$Hard & 00.0 \\
Myopic Greedy & 00.0 \\
Bandit (LinUCB) & 00.0 \\
PBT/BOHB & 00.0 \\
\textbf{PPO Teacher (ours)} & \textbf{00.0} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Out-of-Distribution Transfer (Placeholders)}
\begin{figure}[H]
  \centering
  \fbox{\rule{0pt}{2in}\rule{0.95\linewidth}{0pt}}
  \caption{Zero-shot large-model transfer (e.g., ViT detection) and OOD regions (higher noise/imbalance).}
  \label{fig:ood}
\end{figure}

\begin{table}[H]
\centering
\caption{Zero-shot and few-shot MacroAcc on OOD grid and held-out task families.}
\label{tab:ood}
\begin{tabular}{lcc}
\toprule
Method & Zero-shot $\uparrow$ & Few-shot (10 eps) $\uparrow$ \\
\midrule
Static Uniform & 00.0 & 00.0 \\
Bandit (Thompson) & 00.0 & 00.0 \\
\textbf{PPO Teacher (ours)} & \textbf{00.0} & \textbf{00.0} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablations (Placeholders)}
\begin{itemize}[leftmargin=1.5em]
\item \textbf{State sufficiency:} remove trends; ablate dataset-embedding features or model/progress blocks (e.g., drop ECE or gradient stats); replace with alternative scalars; switch MLP$\to$GRU.
\item \textbf{Action parameterization:} continuous simplex vs. sparse top-$k$; effect of discretization for $\ell,u$.
\item \textbf{Encoder invariance:} shared per-dataset block + pooling vs. flat concatenation.
\item \textbf{Horizon/budget:} vary $H$ and $B_0$; measure stability and sample efficiency.
\end{itemize}

\section{Discussion}
We unify three desiderata for RL-based curriculum teaching in the many-to-one, group-sensitive setting: \emph{Markovity}, \emph{number-of-datasets invariance}, and \emph{task/architecture invariance}. By formalizing hidden-state Markovity under SGD and using a \emph{grouped} observation (dataset-level embeddings + model-complexity + training-progress), we obtain an observation that is strictly Markov when information-preserving and empirically \emph{approximately} Markov otherwise (Def.~\ref{def:approx-markov-tight}). 

DeepSets provides the route to invariance: treating datasets as a set and sharing per-dataset encoders with \emph{mean} pooling guarantees order and cardinality robustness without scale drift as $N$ varies. Coupling this with shared per-dataset action heads and normalization yields mixture policies that seamlessly accommodate unseen $N$ and permutations. Crucially, controlling mixture, usage, and step size enables \MAESTRO{} to \emph{protect minority datasets} by adaptively reallocating budget—an operational complement to distributionally robust objectives.

\paragraph{Implications.}
A compact, task-agnostic grouped observation suffices to control learning dynamics across task families and architectures while preserving Markovity and $N$-invariance. This supports training a single teacher on inexpensive episodes and transferring it to large-scale mixtures where small, high-stakes datasets must not be overshadowed.

\paragraph{Limitations.}
The grouped state is intentionally coarse, foregoing task-specific diagnostics (e.g., error typing in detection) that may accelerate learning in specialized domains. Its efficacy depends on stable computation of NLL and confidence summaries (sequence normalization matters). Extreme regimes (e.g., severe class imbalance with heavy augmentation) may benefit from adding task-agnostic action heads (e.g., augmentation intensity tiers), which we leave to future work.

\section{Conclusion}
We presented a Markov, number-of-datasets invariant, and task/architecture-invariant formulation for RL-based curriculum teaching, aimed at the practical setting where small, critical datasets are trained alongside large auxiliaries. Our grouped state (dataset embeddings + model-complexity + training-progress) and set-based encoders make the problem well-posed for RL and scalable across any number of datasets. By jointly allocating mixture, learning rate, and usage, the teacher steers training toward balanced outcomes—echoing the fairness intuition that small groups should not be ignored—without changing the base loss. 

\medskip\noindent\textbf{Name.} We call this teacher \MAESTRO{}: \emph{Markovian, Architecture-Agnostic, Equitable Scheduling for Task-Robust Optimization}.
\noindent\textbf{Reproducibility.}
We will release code, configs, and scripts to reproduce all experiments (including CI-friendly runs), together with fixed probes and seeds.

% ---------- References ----------
\bibliographystyle{plainnat}
\begin{thebibliography}{30}

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009curriculum}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th International Conference on Machine Learning (ICML)}, 2009.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010selfpaced}
M.~P. Kumar, B.~Packer, and D.~Koller.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2010.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and Silver]{schaul2016prioritized}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Jaderberg et~al.(2017)Jaderberg, et~al.]{jaderberg2017pbt}
M.~Jaderberg et~al.
\newblock Population based training of neural networks.
\newblock \emph{arXiv:1711.09846}, 2017.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
J.~Snoek, H.~Larochelle, and R.~P. Adams.
\newblock Practical Bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2012.

\bibitem[Falkner et~al.(2018)Falkner, Klein, and Hutter]{falkner2018bohb}
S.~Falkner, A.~Klein, and F.~Hutter.
\newblock BOHB: Robust and efficient hyperparameter optimization at scale.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017maml}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Duan et~al.(2016)Duan, et~al.]{duan2016rl2}
Y.~Duan et~al.
\newblock RL$^2$: Fast reinforcement learning via slow reinforcement learning.
\newblock \emph{arXiv:1611.02779}, 2016.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola]{zaheer2017deepsets}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~Salakhutdinov, and A.~Smola.
\newblock Deep Sets.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019settransformer}
J.~Lee, Y.~Lee, J.~Kim, A.~R. Kosiorek, S.~Choi, and Y.~W. Teh.
\newblock Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Fan et~al.(2018)]{fan2018l2t}
Fan et~al.
\newblock Learning to Teach.
\newblock \emph{arXiv:1805.03643}, 2018.

\bibitem[Wu et~al.(2018)]{wu2018l2tloss}
Wu et~al.
\newblock Learning to Teach with Dynamic Loss Functions.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Kendall et~al.(2018)]{kendall2018uncertainty}
A.~Kendall, Y.~Gal, and R.~Cipolla.
\newblock Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2018.

\bibitem[Chen et~al.(2018)]{chen2018gradnorm}
Z.~Chen, V.~Badrinarayanan, C.-Y.~Lee, and A.~Rabinovich.
\newblock GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks.
\newblock \emph{arXiv:1711.02257}, 2018.

\bibitem[Narvekar et~al.(2020)]{narvekar2020survey}
S.~Narvekar, J.~Sinapov, M.~Leonetti, and P.~Stone.
\newblock Curriculum Learning for Reinforcement Learning: A Survey.
\newblock \emph{arXiv:2003.04960}, 2020.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Srivastava, Namkoong, and Liang]{hashimoto2018fairness}
T.~B. Hashimoto, M.~Srivastava, H.~Namkoong, and P.~Liang.
\newblock Fairness Without Demographics in Repeated Loss Minimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Sagawa et~al.(2020)Sagawa, Koh, Hashimoto, and Liang]{sagawa2020dro}
S.~Sagawa, P.~W. Koh, T.~B. Hashimoto, and P.~Liang.
\newblock Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Group Performance.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2020.

\end{thebibliography}

\end{document}

